{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\nimport re\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-08T01:57:41.312674Z","iopub.execute_input":"2022-04-08T01:57:41.313044Z","iopub.status.idle":"2022-04-08T01:57:41.369607Z","shell.execute_reply.started":"2022-04-08T01:57:41.312986Z","shell.execute_reply":"2022-04-08T01:57:41.368853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic overview of dataset","metadata":{}},{"cell_type":"markdown","source":"Submission outline:","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.374466Z","iopub.execute_input":"2022-04-08T01:57:41.374769Z","iopub.status.idle":"2022-04-08T01:57:41.389104Z","shell.execute_reply.started":"2022-04-08T01:57:41.374731Z","shell.execute_reply":"2022-04-08T01:57:41.388612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature dataset","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/features.csv\")\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.390122Z","iopub.execute_input":"2022-04-08T01:57:41.390501Z","iopub.status.idle":"2022-04-08T01:57:41.400317Z","shell.execute_reply.started":"2022-04-08T01:57:41.390472Z","shell.execute_reply":"2022-04-08T01:57:41.399715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features[\"case_num\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.40295Z","iopub.execute_input":"2022-04-08T01:57:41.403359Z","iopub.status.idle":"2022-04-08T01:57:41.409793Z","shell.execute_reply.started":"2022-04-08T01:57:41.403312Z","shell.execute_reply":"2022-04-08T01:57:41.408859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## explaination of each feature:\n1. feature_num, a unique identifier\n2. case_num, a unique number for each case, 即分类类别\n3. feature_text, 命名体识别对应的text\n\n用一个简单的例子查看一下:\n","metadata":{}},{"cell_type":"code","source":"features.iloc[0,:][\"feature_text\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.411022Z","iopub.execute_input":"2022-04-08T01:57:41.411487Z","iopub.status.idle":"2022-04-08T01:57:41.418702Z","shell.execute_reply.started":"2022-04-08T01:57:41.411445Z","shell.execute_reply":"2022-04-08T01:57:41.418068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## patient notes explaination\nA collection of about 40,000 Patient Note history portions. Only a subset of these have features annotated. You may wish to apply unsupervised learning techniques on the notes without annotations. The patient notes in the test set are not included in the public version of this file.\n\n1. pn_num: id， 40000个patient note的对应id\n2. case_num: 不同的case\n3. pn_history: 即对应的文本内容，里面的\\r\\n 即为换行符(windows中特定)","metadata":{}},{"cell_type":"code","source":"patient_notes = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/patient_notes.csv\")\npatient_notes.head().iloc[0][\"pn_history\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.419781Z","iopub.execute_input":"2022-04-08T01:57:41.420282Z","iopub.status.idle":"2022-04-08T01:57:41.771886Z","shell.execute_reply.started":"2022-04-08T01:57:41.42024Z","shell.execute_reply":"2022-04-08T01:57:41.771021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.773201Z","iopub.execute_input":"2022-04-08T01:57:41.774985Z","iopub.status.idle":"2022-04-08T01:57:41.787566Z","shell.execute_reply.started":"2022-04-08T01:57:41.774946Z","shell.execute_reply":"2022-04-08T01:57:41.786916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#最长的就194个词？？？\npatient_notes[\"pn_history\"].apply(lambda x: len(x.split())).max()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:41.788978Z","iopub.execute_input":"2022-04-08T01:57:41.789864Z","iopub.status.idle":"2022-04-08T01:57:42.162526Z","shell.execute_reply.started":"2022-04-08T01:57:41.789829Z","shell.execute_reply":"2022-04-08T01:57:42.16177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CHECK train.csv","metadata":{}},{"cell_type":"markdown","source":"可以看到，14300条数据中有9901条是有结果的","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/train.csv\")\ntrain_1 = train.copy()\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.163611Z","iopub.execute_input":"2022-04-08T01:57:42.163876Z","iopub.status.idle":"2022-04-08T01:57:42.201046Z","shell.execute_reply.started":"2022-04-08T01:57:42.163842Z","shell.execute_reply":"2022-04-08T01:57:42.20022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train[\"annotation\"] != \"[]\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.202258Z","iopub.execute_input":"2022-04-08T01:57:42.202451Z","iopub.status.idle":"2022-04-08T01:57:42.219493Z","shell.execute_reply.started":"2022-04-08T01:57:42.202428Z","shell.execute_reply":"2022-04-08T01:57:42.218837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/test.csv\")\ntest = pd.merge(test, features, on = [\"feature_num\", \"case_num\"], how = \"left\")\ntest = pd.merge(test, patient_notes, on = [\"pn_num\", \"case_num\"], how = \"left\") \ntest","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.220517Z","iopub.execute_input":"2022-04-08T01:57:42.220885Z","iopub.status.idle":"2022-04-08T01:57:42.253004Z","shell.execute_reply.started":"2022-04-08T01:57:42.220857Z","shell.execute_reply":"2022-04-08T01:57:42.252181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merge the dataset together","metadata":{}},{"cell_type":"code","source":"train = pd.merge(train,features, on = [\"feature_num\", \"case_num\"], how = \"left\")\ntrain = pd.merge(train, patient_notes, on = [\"pn_num\", \"case_num\"], how = \"left\") ","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.254227Z","iopub.execute_input":"2022-04-08T01:57:42.254941Z","iopub.status.idle":"2022-04-08T01:57:42.278718Z","shell.execute_reply.started":"2022-04-08T01:57:42.254902Z","shell.execute_reply":"2022-04-08T01:57:42.278058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.281572Z","iopub.execute_input":"2022-04-08T01:57:42.282Z","iopub.status.idle":"2022-04-08T01:57:42.298249Z","shell.execute_reply.started":"2022-04-08T01:57:42.281962Z","shell.execute_reply":"2022-04-08T01:57:42.297738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval)\ntrain['location'] = train['location'].apply(ast.literal_eval)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.299212Z","iopub.execute_input":"2022-04-08T01:57:42.299654Z","iopub.status.idle":"2022-04-08T01:57:42.632247Z","shell.execute_reply.started":"2022-04-08T01:57:42.299625Z","shell.execute_reply":"2022-04-08T01:57:42.631546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do some checking for each word location and feature text, check whether each of them is correctly labling","metadata":{}},{"cell_type":"markdown","source":"Here I save \\r, \\n since when being a new line, it maybe helpful for our model to detect those key info","metadata":{}},{"cell_type":"code","source":"def correct_true_label(annotation, location, raw_text):\n    '''\n    Input is:\n        1. annotation, the corresponding annotation\n        2. location, each location to annotation, assume to be a string \"a b\"\n        3. raw_text, corresponding raw text\n    '''\n    # 将标注错，标注漏的数据correct\n    ls = [\",\",\".\",\";\",\" \",\"\\r\",\"\\n\",\"\\r\\n\"]\n    idx_start = int(location.split(\" \")[0])\n    idx_end = int(location.split(\" \")[1])\n    true_text = raw_text[idx_start:idx_end]\n    \n    \n    # ------------------------------ 这个while loop 判断index start的问题\n    # 对于这个 while loop，判断start 是不是在ls 里面，如果是，则说明其起始并不是字母或者数字\n    # 需要修正为字母，数字\n    while(1):\n        if raw_text[idx_start] in ls:\n            idx_start += 1\n        # 如果start 不为0 而且 start -1 不在ls里面，说明前面有漏标的数据，需要修正 比如 father为原文， 标注成了 ather\n        elif idx_start >0 and raw_text[idx_start - 1] not in ls:\n            idx_start -= 1\n        else:\n            break\n            \n            \n    # --------------------------------------这个while loop 是判断idx end的问题\n    # 首先如果idx end 不为原始数据 raw text 的结尾\n    # 然后判断结尾是否在ls 里面，如果在，说明标注问题，需要往前移一位\n    # 然后判断结尾 +1 是否在ls里面，如果不在，说明标注漏了， 需要往后移一位\n    while(1):\n        #print(idx_end)\n        #print(raw_text[idx_end])\n        if idx_end < len(raw_text):\n            if raw_text[idx_end] in ls:\n    #             print(\"in if\")\n    #             print(raw_text[idx_end])\n                idx_end -=1\n            elif idx_end +1 < len(raw_text) and raw_text[idx_end + 1] not in ls:\n    #             print(\"in elif\")\n    #             print(raw_text[idx_end + 1])\n                idx_end +=1\n            else:\n                break\n        else:\n            if raw_text[idx_end - 1] not in ls:\n                idx_end -=1\n    annotation = raw_text[idx_start:idx_end+1]\n    #print(annotation, idx_start, idx_end+1)\n    return annotation, idx_start, idx_end+1","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.633501Z","iopub.execute_input":"2022-04-08T01:57:42.633727Z","iopub.status.idle":"2022-04-08T01:57:42.644155Z","shell.execute_reply.started":"2022-04-08T01:57:42.633699Z","shell.execute_reply":"2022-04-08T01:57:42.643304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def checking(annotation, location, raw_text):\n    assert len(annotation) == len(location), \"sth wrong, check the length!\"\n    correct_format = []\n    correct_loc = []\n#     print(annotation)\n\n#----------------- 这里是返回更新后的最新的 location 和 annotation\n    for anno, idx in zip(annotation, location):\n        #print(idx)\n        if \";\" in idx:\n            for j in idx.split(\";\"):\n                count = 0\n                corr_anno, start,end = correct_true_label(anno, j, raw_text)\n                #print(count)\n                #print(corr_anno, start, end)\n                correct_format.append(corr_anno[count:count+int(end) - int(start)])\n                correct_loc.append([start, end])\n                count += int(start) - int(end)+1\n            # count the correct index for annotation\n        else:\n            corr_anno, start,end = correct_true_label(anno, idx, raw_text)\n            correct_loc.append([start, end])\n            correct_format.append(corr_anno)\n    assert len(correct_format) == len(correct_loc)\n    # check the len\n#     print(correct_format)\n#     print(correct_loc)\n\n    # ------------------------ 这里是检查选取出来的长度和实际的长度是否相同\n     # ----------------- 检查选取出来的结果和 实际是否对应\n    tmp =True\n    for anno, loc in zip(correct_format,correct_loc):\n#         print(anno, loc)\n#         print(len(anno), int(loc[1]) - int(loc[0]))\n        len_ = int(loc[1]) - int(loc[0])\n        if len_ != len(anno):\n            tmp = False\n        # 比较完长度之后，从对应的原文中选出对应的值，查看两个string是否相等\n        if raw_text[int(loc[0]):int(loc[1])] != anno:\n            tmp = False\n    # 检查完确认没问题后，返回对应的全新的 annotation 和 location\n    return {\"annotation\":correct_format, \"location\":correct_loc}","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.645376Z","iopub.execute_input":"2022-04-08T01:57:42.645844Z","iopub.status.idle":"2022-04-08T01:57:42.660073Z","shell.execute_reply.started":"2022-04-08T01:57:42.645803Z","shell.execute_reply":"2022-04-08T01:57:42.659485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_feature(text):\n    '''\n    Clean the feature text\n    '''\n    text = re.sub(\"-OR-\", \" or \", text)\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub(\"-\", \" \", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.661035Z","iopub.execute_input":"2022-04-08T01:57:42.661343Z","iopub.status.idle":"2022-04-08T01:57:42.673521Z","shell.execute_reply.started":"2022-04-08T01:57:42.661318Z","shell.execute_reply":"2022-04-08T01:57:42.672944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"feature_text\"] = train.apply(lambda x: clean_feature(x[\"feature_text\"]), axis = 1)\ntest[\"feature_text\"] = test.apply(lambda x: clean_feature(x[\"feature_text\"]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.674529Z","iopub.execute_input":"2022-04-08T01:57:42.674955Z","iopub.status.idle":"2022-04-08T01:57:42.941654Z","shell.execute_reply.started":"2022-04-08T01:57:42.674901Z","shell.execute_reply":"2022-04-08T01:57:42.940961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = train.apply(lambda x: checking(x[\"annotation\"], x[\"location\"], x[\"pn_history\"]), axis=1)\ntrain[\"annotation\"] = tmp.apply(lambda x: x[\"annotation\"])\ntrain[\"location\"] = tmp.apply(lambda x: x[\"location\"])\n# check how many answer for specific question\ntrain[\"annotation_length\"] = train.apply(lambda x: len(x[\"annotation\"]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:42.942794Z","iopub.execute_input":"2022-04-08T01:57:42.943098Z","iopub.status.idle":"2022-04-08T01:57:43.693608Z","shell.execute_reply.started":"2022-04-08T01:57:42.943061Z","shell.execute_reply":"2022-04-08T01:57:43.692804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:57:43.69471Z","iopub.execute_input":"2022-04-08T01:57:43.694926Z","iopub.status.idle":"2022-04-08T01:57:43.720388Z","shell.execute_reply.started":"2022-04-08T01:57:43.694901Z","shell.execute_reply":"2022-04-08T01:57:43.719766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-30T13:25:12.769898Z","iopub.execute_input":"2022-03-30T13:25:12.770088Z","iopub.status.idle":"2022-03-30T13:25:15.115052Z","shell.execute_reply.started":"2022-03-30T13:25:12.770062Z","shell.execute_reply":"2022-03-30T13:25:15.1144Z"}}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModel","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:22.05765Z","iopub.execute_input":"2022-04-08T01:58:22.057937Z","iopub.status.idle":"2022-04-08T01:58:23.417644Z","shell.execute_reply.started":"2022-04-08T01:58:22.057909Z","shell.execute_reply":"2022-04-08T01:58:23.41666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tokenizer(name, precompute=False, df=None, path=None):\n    '''\n    \n    '''\n    if path is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(path)\n\n    tokenizer.name = name\n\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed=None\n        \n    return tokenizer\n\ndef precompute_tokens(df, tokenizer):\n    \n    feature_text = df[\"feature_text\"]\n    idx = {}\n    offsets = {}\n    for txt in feature_text:\n        encoding = tokenizer(\n            txt,\n            return_token_type_ids = True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        idx[txt] = encoding[\"input_ids\"]\n        # character Index\n        offsets[txt] = encoding[\"offset_mapping\"]\n        \n    for txt in df[\"pn_history\"].unique():\n        encoding = tokenizer(\n            txt,\n            return_token_type_ids = True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        idx[txt] = encoding[\"input_ids\"]\n        offsets[txt] = encoding[\"offset_mapping\"]\n    return {\"idx\":idx, \"offsets\":offsets}\n    \ndef robert_process(feature_text, text, pre_computed, tokenizer, max_length = 300, location = None, anno_len = None):\n    '''\n    Preprocess it as a Question answering problem, where question is feature_text, answer in text file\n    \n    This function will change the original labels to token labels:\n    For this kind of question_answering problem:\n    Notice that:\n        1. For each question(which is feature_text here), have multiple discrete solutions, [[123,124], [145, 146]] for example\n        2. We cannot just map it as start logit and end logit, which means we will use the whole length as label: if our text length is 5, then the lables will like this:\n            [0,1,1,-1,-1], where 0 means not in answer, 1 means in answer, -1 means padding\n    '''\n    tokens = tokenizer.special_tokens\n    sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    # for input_ids\n    input_ids = [tokens[\"cls\"]] + pre_computed[\"idx\"][feature_text] + sep\n    question_len = len(input_ids)\n    #print(question_len)\n    # add answer text\n    input_ids += pre_computed[\"idx\"][text] \n    #print(len(input_ids))\n    input_ids = input_ids[:max_length -1] + [tokens[\"sep\"]]\n    # segment id\n    # since roberta don't have tokens id, so just all 0 is ok\n    token_type_ids = [0] * len(input_ids)\n    \n    # for character index\n    # don't care feature text, only for answer text\n    offsets = [(0,0)] * question_len + pre_computed[\"offsets\"][text]\n    # for end \n    offsets = offsets[:max_length -1] + [(0,0)]\n    \n    # add the padding\n    padding_length = max_length - len(input_ids)\n    \n    if padding_length > 0 :\n        input_ids +=  [tokens[\"pad\"]] * padding_length\n        token_type_ids += [0] * padding_length\n        offsets += [(0,0)] * padding_length\n    #print(offsets)\n    # create labels\n    if location is not None:\n        labels = np.zeros_like(input_ids)\n        # set all question to 0 , special token, padding to -1\n        labels[0] = -1\n        labels[1:question_len] = 0\n        labels[question_len] = -1\n        labels[question_len-1] = -1\n        labels[max_length - padding_length-1:] = -1\n        if anno_len != 0:\n            for loc in location:\n                    start_idx = -1\n                    end_idx = -1\n                    start, end = int(loc[0]), int(loc[1])\n                    #print(start, end)\n                    for idx in range(len(offsets)):\n                        #choose the index \n                        if (start_idx == -1) & (start < offsets[idx][0]):\n                            start_idx = idx - 1\n                        if (end_idx == -1) & (end <= offsets[idx][1]):\n                            end_idx = idx + 1\n                    if start_idx == -1:\n                        start_idx = end_idx\n                    if (start_idx != -1) & (end_idx != -1):\n                        labels[start_idx:end_idx] = 1\n            #print(start, end, start_idx, end_idx)\n                 \n        assert len(input_ids) == len(token_type_ids) == len(offsets) == max_length == len(labels)\n        return {\n            \"input_ids\": input_ids,\n            \"segment_ids\":token_type_ids,\n            \"offsets\": offsets,\n            \"labels\":labels\n        }\n    assert len(input_ids) == len(token_type_ids) == len(offsets) == max_length\n    return  {\n            \"input_ids\": input_ids,\n            \"segment_ids\":token_type_ids,\n            \"offsets\": offsets\n            }","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:23.419578Z","iopub.execute_input":"2022-04-08T01:58:23.419958Z","iopub.status.idle":"2022-04-08T01:58:23.443468Z","shell.execute_reply.started":"2022-04-08T01:58:23.419923Z","shell.execute_reply":"2022-04-08T01:58:23.442524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer(\"roberta-large\", True, train, \"../input/roberta-baseline-params\")\ntokenizer_test = get_tokenizer(\"roberta-large\", True, test, \"../input/roberta-baseline-params\")","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:23.444973Z","iopub.execute_input":"2022-04-08T01:58:23.445264Z","iopub.status.idle":"2022-04-08T01:58:25.53858Z","shell.execute_reply.started":"2022-04-08T01:58:23.445225Z","shell.execute_reply":"2022-04-08T01:58:25.537649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_max_length(feature_text, text, pre_computed, tokenizer):\n    tokens = tokenizer.special_tokens\n    sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    # for input_ids\n    input_ids = [tokens[\"cls\"]] + pre_computed[\"idx\"][feature_text] + sep\n    #print(question_len)\n    # add answer text\n    input_ids += pre_computed[\"idx\"][text] \n    #print(len(input_ids))\n    input_ids = input_ids + [tokens[\"sep\"]]\n    return len(input_ids)\nmax_length = train.apply(lambda x:check_max_length(x[\"feature_text\"], x[\"pn_history\"], tokenizer.precomputed, tokenizer), axis=1).max()\nmax_length","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:25.540488Z","iopub.execute_input":"2022-04-08T01:58:25.540793Z","iopub.status.idle":"2022-04-08T01:58:25.875149Z","shell.execute_reply.started":"2022-04-08T01:58:25.540743Z","shell.execute_reply":"2022-04-08T01:58:25.874135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create torch dataset","metadata":{}},{"cell_type":"code","source":"import torch\nclass Patient_note(torch.utils.data.Dataset):\n    def __init__(self, train_set, tokenizer, max_length):\n        self.df = train_set\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        text = self.df.iloc[idx][\"pn_history\"]\n        feature_text = self.df.iloc[idx][\"feature_text\"]\n        anno_len = self.df.iloc[idx][\"annotation_length\"]\n        location = self.df.iloc[idx][\"location\"]\n        #print(location)\n        encoding = robert_process(feature_text, text,self.tokenizer.precomputed, self.tokenizer, self.max_length, location, anno_len)\n        return torch.tensor(encoding[\"input_ids\"]), torch.tensor(encoding[\"segment_ids\"]), torch.tensor(encoding[\"labels\"]) ,np.array(encoding[\"offsets\"]), text\ntrain_Set = Patient_note(train, tokenizer, max_length)\n_,_, labels, offset, _ = next(iter(train_Set))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:28.962404Z","iopub.execute_input":"2022-04-08T01:58:28.96269Z","iopub.status.idle":"2022-04-08T01:58:28.98823Z","shell.execute_reply.started":"2022-04-08T01:58:28.962658Z","shell.execute_reply":"2022-04-08T01:58:28.987665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Test_dataset(torch.utils.data.Dataset):\n    def __init__(self, test_set, tokenizer, max_length):\n        self.df = test_set\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        text = self.df.iloc[idx][\"pn_history\"]\n        feature_text = self.df.iloc[idx][\"feature_text\"]\n        encoding = robert_process(feature_text, text,self.tokenizer.precomputed, self.tokenizer, self.max_length)\n        return torch.tensor(encoding[\"input_ids\"]), torch.tensor(encoding[\"segment_ids\"]), np.array(encoding[\"offsets\"])\ntest_set = Test_dataset(test, tokenizer_test, max_length)\n#next(iter(test_set))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:40.384103Z","iopub.execute_input":"2022-04-08T01:58:40.384768Z","iopub.status.idle":"2022-04-08T01:58:40.393256Z","shell.execute_reply.started":"2022-04-08T01:58:40.384716Z","shell.execute_reply":"2022-04-08T01:58:40.392654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nclass Roberta_baseline(nn.Module):\n    def __init__(self, model_name, num_class, pretrained = True, path = None):\n        super().__init__()\n        # basic model from transformers\n        if pretrained:\n            self.model = AutoModel.from_pretrained(model_name)\n        else:\n            self.model = AutoModel.from_pretrained(path)\n        self.ln1 = nn.Linear(1024, num_class)\n        self.padding_idx = 1\n    def forward(self, input_ids, segment_ids):\n        # get last hidden state as output, which is (batch_size, seq_len, hidden_size)\n        logit = self.model(\n            input_ids,\n            attention_mask = (input_ids != self.padding_idx).long(),\n            token_type_ids = segment_ids)[0]\n       # print(logit.shape)\n        logit = self.ln1(logit)\n        return logit","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:58:44.425794Z","iopub.execute_input":"2022-04-08T01:58:44.426201Z","iopub.status.idle":"2022-04-08T01:58:44.43255Z","shell.execute_reply.started":"2022-04-08T01:58:44.426172Z","shell.execute_reply":"2022-04-08T01:58:44.43204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = Roberta_baseline(\"roberta-large\", 1, False,\"../input/roberta-baseline-params\")\n# dic = torch.load(\"../input/roberta-model-para/model.params\")\n# net.load_state_dict(dic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(predictions, offset_mapping, tolerance = 0.5):\n    '''\n    This function is the post processing after inference from trained model:\n    input params:\n        1. offset_mapping: the result index\n        2. tolerance, the sigmoid tolerance\n    '''\n    result = []\n    for i in range(predictions.shape[0]):\n        idx = np.argwhere(predictions[i,:,:]>tolerance)\n    #print(idx[:,1])\n    #print(offset_mapping.shape)\n        result.append(offset_mapping[i,idx[:,0],:])\n    return result\ndef inference(net, test_set, device):\n    '''\n    input params:\n        1. net, trained net\n        2. test_set\n        3. device\n    '''\n    # since we only have 5 test sample\n    result = []\n    test_iter = torch.utils.data.DataLoader(test_set, batch_size = 5)\n    net.eval()\n    net.to(device)\n    for idx, value in tqdm(enumerate(test_iter)):\n        input_ids = value[0].to(device)\n        seg_ids = value[1].to(device)\n        with torch.no_grad():\n            y_preds = net(input_ids, seg_ids)\n        result.append(post_process(y_preds.sigmoid().to('cpu').numpy(), value[2].to(\"cpu\").numpy()))\n    return result\n# predictions = inference(net, test_set, torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:59:18.678784Z","iopub.execute_input":"2022-04-08T01:59:18.679318Z","iopub.status.idle":"2022-04-08T01:59:18.688415Z","shell.execute_reply.started":"2022-04-08T01:59:18.67927Z","shell.execute_reply":"2022-04-08T01:59:18.687933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_return(predictions):\n    '''\n    This function receive the prediction and will return the merge index.\n    input_prams:\n        predictions: a list with test data\n    '''\n    result = []\n    for prediction in predictions:\n        for i in range(len(prediction)):\n            preds = prediction[i]\n            ls = []\n            if preds.tolist() != []:\n                #print(preds, type(preds))\n                start, end = preds[0,0], preds[0,1]\n                for j in range(preds.shape[0]):\n                    if j+1 >= len(preds):\n                        break\n                    else:\n                        if preds[j+1,0] - preds[j, 1] == 1 or preds[j+1,0] - preds[j, 1] == 0:\n                            end = preds[j+1, 1]\n                            continue\n                        else:\n                            ls.append(str(start)+\" \"+str(end))\n                            start = preds[j+1, 0]\n                            end = preds[j+1, 1]\n                ls.append(str(start)+ \" \" + str(end))\n                if ls == [str(start)+ \" \" + str(end)]:\n                    result.append(str(start) + \" \" + str(end))\n                else:\n                    if start - int(ls[-2].split(\" \")[-1]) == 1 or start - int(ls[-2].split(\" \")[-1]) == 0:\n                        ls[-1] = ls[-2].split(\" \")[0] + \" \" + str(end)\n                        result.append(\";\".join(ls))\n                    else:\n                        result.append(\";\".join(ls))\n            else:\n                result.append(np.NaN)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:59:21.325261Z","iopub.execute_input":"2022-04-08T01:59:21.32593Z","iopub.status.idle":"2022-04-08T01:59:21.338073Z","shell.execute_reply.started":"2022-04-08T01:59:21.325891Z","shell.execute_reply":"2022-04-08T01:59:21.337521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = pd.Series(get_return(predictions))\n# results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv(\"../input/nbme-score-clinical-patient-notes/sample_submission.csv\")\n# submission[\"location\"] = results\n# display(submission.head())\n# submission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AMP 训练\n1. AMP 是一种自动混合精度，称为 Automatic mixed precision, 目的是可以在神经网络推理过程中针对不同的层采用不同的数据精度计算， 可以节省显存加快速度。torch 中两种浮点型Tensor， float 16 和float 32。  \n2. 为何需要AMP？ 因为HalfTensor 存储小，计算快 可以更好的利用CUDA设备的Tensor Core， 训练的时候可以很好的减小显存占用。因此训练的时候可以减少显存的占用（可以增加batchsize了），同时训练速度更快；\n3. torch.HalfTensor的劣势就是：数值范围小（更容易Overflow / Underflow）、舍入误差（Rounding Error，导致一些微小的梯度信息达不到16bit精度的最低分辨率，从而丢失）\n","metadata":{}},{"cell_type":"markdown","source":"## Use BCELosswithlogit as loss function (binary cross entropy loss)\nThis class contains a sigmod+BCEloss:\n$$Equation = \\frac{1}{N}\\sum_{n=1}^{N}l_n$$\nwhere $l_n = -\\omega[y_n * logx_n + (1-y_n) * log(1- x_n)]$, 其中w 是超参数","metadata":{}},{"cell_type":"markdown","source":"## 使用混合精度训练， 即 autocast + GradScaler","metadata":{}},{"cell_type":"markdown","source":"## 一些训练trick\n1. gradient_accumulate_step: 如果显存不足，可以通过梯度累计来解决，举个例子， 假设原来的batch_size = 10, 数据总量为1000， 则需要100次training step，如果这样显存不够，则减小batch_size, 设置gradient_accumulate_step = 2, 则新更新的batch_size = 10/2 = 5, 则此时梯度更新的次数仍是100次， 则最终的training step = 200.\n2. 指定对应的参数更新，比如weight decay不decay 哪一些层， 选定特定的层来更新\n3. scheduler, 代表 根据batch 来进行学习率的调整","metadata":{}},{"cell_type":"code","source":"def get_parameters_function(net, encoder_lr, decoder_lr, weight_decay):\n    '''\n    input_params: \n        1. net, the original network;\n        2. encoder_lr, learning rate for encoder;\n        3. decoder_lr, learning rate for decoder;\n        4. weight_decay, decay rate\n    Notice that here we write judge whole model as encoder and decoder, here Roberta is encoder, linear output is decoder.\n    '''\n    params = list(net.named_parameters())\n    # point out value not need to decay\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_params = [\n        # for encoder, here is Roberta info\n        {\"params\": [p for n, p in net.model.named_parameters() if not any(nd in n for nd in no_decay)], \n        \"lr\": encoder_lr, \"weight_decay\":weight_decay},\n        {\"params\": [p for n, p in net.model.named_parameters() if any(nd in n for nd in no_decay)], \n        \"lr\": encoder_lr, \"weight_decay\":0.0},\n        # for decoder\n        {\"params\":[p for n, p in net.named_parameters() if \"model\" not in n], \n        \"lr\": decoder_lr, \"weight_decay\":0.0}\n    ]\n    return optimizer_params","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:59:26.192205Z","iopub.execute_input":"2022-04-08T01:59:26.192461Z","iopub.status.idle":"2022-04-08T01:59:26.19965Z","shell.execute_reply.started":"2022-04-08T01:59:26.192419Z","shell.execute_reply":"2022-04-08T01:59:26.199059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import get_cosine_schedule_with_warmup\ndef get_scheduler_info(optimizer, train_step):\n    '''\n    This function is the to return the corresponding scheduler:\n    input params:\n        1. optimizer, which is the optimizer that need to be changed the learning rate\n        2. train_step, which is called warm step, means don't change the lr after few epoches\n    warm up 的作用： 对于刚开始训练的模型，由于weight是随机初始化的，若学习率过大，可能会导致震荡等因素的存在，所以在开始的时候，\n    会让学习率从0不断上升，在这一段比较小的学习率下，模型可以慢慢趋于稳定，从而慢慢达到预设的初始学习率， 然后在慢慢下降初始学习率， 进行参数的学习\n    '''\n    # here use cos scheduler from hugging face\n    scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 100, num_training_steps= train_step, num_cycles = 0.5)\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:59:26.618988Z","iopub.execute_input":"2022-04-08T01:59:26.619857Z","iopub.status.idle":"2022-04-08T01:59:31.254004Z","shell.execute_reply.started":"2022-04-08T01:59:26.619814Z","shell.execute_reply":"2022-04-08T01:59:31.253233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_labels(labels):\n    tmp = True\n    for i in range(labels.shape[0]):\n        #print(np.isin(labels[i,:].detach().numpy(),1).any())\n        if not np.isin(labels[i,:].detach().numpy(),1).any():\n            tmp = False\n            return tmp\n    return tmp","metadata":{"execution":{"iopub.status.busy":"2022-04-08T01:59:31.255691Z","iopub.execute_input":"2022-04-08T01:59:31.256015Z","iopub.status.idle":"2022-04-08T01:59:31.26237Z","shell.execute_reply.started":"2022-04-08T01:59:31.255972Z","shell.execute_reply":"2022-04-08T01:59:31.261331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For this dataset\nfrom torch.cuda import amp\ndef train_with_amp(net, train_set, criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm ,device):\n    net.train()\n    # instantiate a scalar object  \n    print(\"train on \" + str(device))\n    ls = []\n    enable_amp = True if \"cuda\" in device.type else False\n    scaler = amp.GradScaler(enabled= enable_amp)\n    net.to(device)\n    global_step = 0\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n    for epoch in range(epochs):\n        for idx, value in enumerate(train_iter):\n            #sig = check_labels(value[2])\n            input_ids = value[0].to(device)\n            seg_ids = value[1].to(device)\n            labels = value[2].to(device)\n            #print(sig)\n            # when forward process, use amp\n            with amp.autocast(enabled= enable_amp):\n                output = net(input_ids, seg_ids)\n            loss = criterion(output.view(-1,1), labels.view(-1,1).float())\n                # do the selection for non -1 data\n            loss =  torch.masked_select(loss, labels.view(-1,1) != -1).mean()\n            # prevent gradient to 0\n            if gradient_accumulate_step > 1:\n                # 如果显存不足，通过 gradient_accumulate 来解决\n                loss = loss/gradient_accumulate_step\n            \n            # 放大梯度，避免其消失\n            scaler.scale(loss).backward()\n            # do the gradient clip\n            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n            if (idx + 1) % gradient_accumulate_step == 0:\n                # 多少 step 更新一次梯度\n                # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n                # 否则忽略step调用， 保证权重不更新\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                global_step += 1\n                scheduler.step()\n            # 每100次计算 print 出一次loss\n            if idx % 1000 == 0 or idx == len(train_iter) -1:\n                with torch.no_grad():\n                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n                    print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n                    torch.save({\"net\":net.state_dict(), \"epoches\": epochs, \"optimizer\": optimizer.state_dict()}, \"./model.params\")\n                    ls.append(loss.item())\n            #print(\"successfully done one train\")\n    # Draw the loss\n    import matplotlib.pyplot as plt\n    plt.figure()\n    plt.plot(ls)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T02:09:33.950768Z","iopub.execute_input":"2022-04-08T02:09:33.951034Z","iopub.status.idle":"2022-04-08T02:09:33.964344Z","shell.execute_reply.started":"2022-04-08T02:09:33.951007Z","shell.execute_reply":"2022-04-08T02:09:33.9638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_train(train_set, valid_Set):\n    encoder_lr, decoder_lr = 2e-6, 2e-6\n    weight_decay, eps = 0.01, 1e-6\n    net = Roberta_baseline(\"roberta-large\", 1, False,\"../input/roberta-baseline-params\")\n    #dic = torch.load(\"../input/roberta-model-para/model.params\")\n   # net.load_state_dict(dic)\n    num_class = 1\n    num_epoches = 5\n    batch_size = 6\n#     name = \"roberta-large\"\n#     net = Roberta_baseline(name, num_class)    \n    optim_param = get_parameters_function(net, encoder_lr, decoder_lr, weight_decay)\n    optimizer = torch.optim.AdamW(params= optim_param, lr = encoder_lr, eps=eps)\n    # Since we will do some selection after loss function, just set reduction to None\n    # And we process it by ourselves\n    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")    \n    scheduler = get_scheduler_info(optimizer, len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)))\n    train_with_amp(net, train_set, criterion, optimizer, num_epoches, batch_size, scheduler, 1,1500,torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-08T02:09:34.3184Z","iopub.execute_input":"2022-04-08T02:09:34.318968Z","iopub.status.idle":"2022-04-08T02:09:34.326425Z","shell.execute_reply.started":"2022-04-08T02:09:34.318934Z","shell.execute_reply":"2022-04-08T02:09:34.325934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train(train_Set, None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}